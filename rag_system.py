#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ìƒˆë¡œìš´ RAG ì‹œìŠ¤í…œ
SFR + FAISS + Llama 3.2 8bë¥¼ ì‚¬ìš©í•œ ì§ˆë¬¸ ë‹µë³€ ì‹œìŠ¤í…œ
"""

import os
import json
import pickle
import numpy as np
import torch
import faiss
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from typing import List, Dict, Optional
import logging
import re


class NewRAGSystem:
    def __init__(self, faiss_dir: str = "./crawling/faiss_output", device_id: int = 0):
        """
        ìƒˆë¡œìš´ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            faiss_dir: FAISS ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬
            device_id: ì‚¬ìš©í•  GPU ID (0ë²ˆ GPU ê³ ì •)
        """
        self.faiss_dir = faiss_dir
        self.device = f"cuda:{device_id}"
        
        # ëª¨ë¸ë“¤
        self.sfr_model = None
        self.llama_tokenizer = None
        self.llama_model = None
        
        # FAISS ê´€ë ¨
        self.faiss_index = None
        self.texts = []
        self.metadata = []
        self.config = {}
        
        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(__name__)
        
    def load_embedding_model(self):
        """SFR ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        self.logger.info("SFR ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # SFR ëª¨ë¸ (Salesforce/SFR-Embedding-Mistral)
        model_name = "Salesforce/SFR-Embedding-Mistral"
        self.sfr_model = SentenceTransformer(model_name, device=self.device)
        
        self.logger.info(f"âœ… SFR ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
    
    def load_llama_model(self):
        """Llama 3.2 ëª¨ë¸ ë¡œë“œ (ë” ê°€ë²¼ìš´ ë²„ì „ ì‚¬ìš©)"""
        self.logger.info("Llama 3.2 ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # ë” ê°€ë²¼ìš´ ëª¨ë¸ ì‚¬ìš© (1B ë˜ëŠ” 3B)
        model_name = "meta-llama/Meta-Llama-3-8B-Instruct"  # 8B ëŒ€ì‹  1B ì‚¬ìš©
        
        # ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´)
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.llama_tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        
        if self.llama_tokenizer.pad_token is None:
            self.llama_tokenizer.pad_token = self.llama_tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë“œ (0ë²ˆ GPUì— ê³ ì •)
        self.llama_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map={"": self.device},  # 0ë²ˆ GPUì— ê³ ì •
            trust_remote_code=True,
            torch_dtype=torch.bfloat16
        )
        
        self.logger.info(f"âœ… Llama 3.2 1B ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (GPU: {self.device})")
    
    def load_faiss_index(self):
        """FAISS ì¸ë±ìŠ¤ì™€ ê´€ë ¨ ë°ì´í„° ë¡œë“œ"""
        self.logger.info("FAISS ì¸ë±ìŠ¤ ë¡œë”© ì¤‘...")
        
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        config_path = os.path.join(self.faiss_dir, "config.json")
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        index_path = os.path.join(self.faiss_dir, "faiss_index.bin")
        self.faiss_index = faiss.read_index(index_path)
        
        # í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        texts_path = os.path.join(self.faiss_dir, "texts.pkl")
        with open(texts_path, 'rb') as f:
            self.texts = pickle.load(f)
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_path = os.path.join(self.faiss_dir, "metadata.pkl")
        with open(metadata_path, 'rb') as f:
            self.metadata = pickle.load(f)
        
        self.logger.info(f"âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {self.faiss_index.ntotal}ê°œ ë²¡í„°, {len(self.texts)}ê°œ í…ìŠ¤íŠ¸")
    
    def initialize_all(self):
        """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        self.logger.info("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
        
        try:
            self.load_embedding_model()
            self.load_faiss_index() 
            self.load_llama_model()
            
            self.logger.info("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def search_similar_documents(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        ì¿¼ë¦¬ì— ëŒ€í•œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if self.sfr_model is None or self.faiss_index is None:
            raise ValueError("RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
        # ì¿¼ë¦¬ë¥¼ SFRë¡œ ì„ë² ë”©
        query_embedding = self.sfr_model.encode([query])
        
        # FAISS ê²€ìƒ‰
        distances, indices = self.faiss_index.search(
            query_embedding.astype('float32'), top_k
        )
        
        # ê²€ìƒ‰ ê²°ê³¼ êµ¬ì„±
        results = []
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            if idx < len(self.texts):
                result = {
                    'rank': i + 1,
                    'text': self.texts[idx],
                    'metadata': self.metadata[idx] if idx < len(self.metadata) else {},
                    'distance': float(distance),
                    'similarity_score': 1 / (1 + distance)
                }
                results.append(result)
        
        return results
    
    def generate_answer_with_llama(self, question: str, context_docs: List[Dict]) -> str:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ Llama 3.2 8bë¡œ ë‹µë³€ ìƒì„±
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            context_docs: ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œë“¤
            
        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        if self.llama_model is None or self.llama_tokenizer is None:
            raise ValueError("Llama ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_text = ""
        for i, doc in enumerate(context_docs[:3], 1):  # ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
            text = doc['text'][:300]  # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
            context_text += f"[ì°¸ê³ ìë£Œ {i}]\n{text}\n\n"
        
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ìƒë“¤ì„ ë•ëŠ” ì¹œê·¼í•œ AI ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì•„ë˜ ì°¸ê³  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.

ì¤‘ìš”í•œ ê·œì¹™:
0. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”
2. ì ˆëŒ€ë¡œ ì½”ë“œ, í”„ë¡œê·¸ë˜ë° ì–¸ì–´, í•¨ìˆ˜, ë³€ìˆ˜ëª… ë“±ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
3. ì¼ë°˜ì ì¸ ëŒ€í™”ì²´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”
4. ì°¸ê³  ì •ë³´ì˜ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
5. ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”
6. ë‹µë³€ì€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”

ì°¸ê³  ì •ë³´:
***
{context_text}
***

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        
        # í† í¬ë‚˜ì´ì €ë¡œ ë³€í™˜
        inputs = self.llama_tokenizer(
            prompt, 
            return_tensors="pt", 
            max_length=2048,
            truncation=True
        ).to(self.device)
        
        # ë‹µë³€ ìƒì„±
        with torch.no_grad():
            outputs = self.llama_model.generate(
                **inputs,
                max_new_tokens=300,  # ë” ì§§ê²Œ ì¡°ì •
                temperature=0.3,  # ë” ë³´ìˆ˜ì ìœ¼ë¡œ ë³€ê²½
                do_sample=True,
                top_p=0.8,  # ë” ë³´ìˆ˜ì ìœ¼ë¡œ
                top_k=40,
                repetition_penalty=1.2,  # ë” ì—„ê²©í•˜ê²Œ
                pad_token_id=self.llama_tokenizer.eos_token_id,
                eos_token_id=self.llama_tokenizer.eos_token_id
            )
        
        # ì‘ë‹µ ë””ì½”ë”©
        generated_text = self.llama_tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        ).strip()
        
        # ë‹µë³€ í›„ì²˜ë¦¬
        generated_text = self._post_process_answer(generated_text)
        
        return generated_text
    
    def _post_process_answer(self, text: str) -> str:
        """
        ìƒì„±ëœ ë‹µë³€ì„ í›„ì²˜ë¦¬í•˜ì—¬ í’ˆì§ˆì„ ê°œì„ 
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            í›„ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        if not text or len(text.strip()) < 3:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ê¸°ë³¸ì ì¸ ì •ë¦¬
        text = text.strip()
        
        # ì½”ë“œ ë¸”ë¡ ì œê±° (```ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¶€ë¶„)
        text = re.sub(r'```[\s\S]*?```', '', text)
        text = re.sub(r'`[^`]*`', '', text)  # ì¸ë¼ì¸ ì½”ë“œ ì œê±°
        
        # í”„ë¡œê·¸ë˜ë° ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì¤„ ì œê±°
        programming_keywords = [
            'import', 'def ', 'class ', 'return', 'if __name__',
            'python', 'function', 'variable', '()', '{','}', 
            'def(', 'return(', 'import ', 'from ', 'print(',
            '= [', '= {', '= (', 'lambda', 'yield'
        ]
        
        lines = text.split('\n')
        clean_lines = []
        
        for line in lines:
            line = line.strip()
            # ë¹ˆ ì¤„ì´ë‚˜ ë„ˆë¬´ ì§§ì€ ì¤„ ê±´ë„ˆë›°ê¸°
            if not line or len(line) < 3:
                continue
                
            # í”„ë¡œê·¸ë˜ë° í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì¤„ ì œê±°
            contains_code = any(keyword.lower() in line.lower() for keyword in programming_keywords)
            
            # ì˜ì–´ë¡œë§Œ êµ¬ì„±ëœ ì¤„ ì œê±° (í•œêµ­ì–´ê°€ ì—†ëŠ” ê²½ìš°)
            has_korean = any('\uac00' <= char <= '\ud7af' for char in line)
            
            if not contains_code and (has_korean or len([c for c in line if c.isalpha()]) < len(line) * 0.5):
                clean_lines.append(line)
        
        # ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
        text = ' '.join(clean_lines)
        
        # íŠ¹ìˆ˜ ë¬¸ìë‚˜ ì´ìƒí•œ íŒ¨í„´ ì œê±°
        text = re.sub(r'[(){}\[\]<>]', '', text)  # ê´„í˜¸ ì œê±°
        text = re.sub(r'\s+', ' ', text)  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = text.strip()
        
        # ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ ì—†ëŠ” ë‹µë³€ ì²˜ë¦¬
        if len(text) < 10:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ëª…í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì‹œê² ì–´ìš”?"
        
        # ê¸¸ì´ ì œí•œ (400ì ì´ë‚´)
        if len(text) > 400:
            # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ê¹Œì§€ë§Œ ìë¥´ê¸°
            sentences = text.split('.')
            result = ""
            for sentence in sentences:
                if len(result + sentence + '.') <= 400:
                    result += sentence + '.'
                else:
                    break
            text = result if result else text[:400] + "..."
        
        # ë§ˆì§€ë§‰ ì •ë¦¬
        if not text.endswith(('.', '!', '?', 'ìš”', 'ë‹¤', 'ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤')):
            if '.' not in text[-10:]:
                text += "."
        
        return text
    
    def generate_rag_answer(self, question: str) -> str:
        """
        RAG íŒŒì´í”„ë¼ì¸: ê²€ìƒ‰ + ìƒì„±
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            RAGë¡œ ìƒì„±ëœ ë‹µë³€
        """
        try:
            # 1. FAISS ê²€ìƒ‰
            search_results = self.search_similar_documents(question, top_k=3)
            
            if not search_results:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # 2. Llamaë¡œ ë‹µë³€ ìƒì„±
            answer = self.generate_answer_with_llama(question, search_results)
            
            # 3. ë¡œê¹…
            self.logger.info(f"RAG ë‹µë³€ ìƒì„± ì™„ë£Œ - ì§ˆë¬¸: {question[:50]}...")
            for i, result in enumerate(search_results, 1):
                self.logger.debug(f"  ì°¸ê³ ìë£Œ {i}: ìœ ì‚¬ë„ {result['similarity_score']:.3f}")
            
            return answer
            
        except Exception as e:
            self.logger.error(f"RAG ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def generate_rag_answer_with_documents(self, question: str) -> Dict[str, any]:
        """
        RAG íŒŒì´í”„ë¼ì¸: ê²€ìƒ‰ + ìƒì„± + ë¬¸ì„œ ë§í¬ ë°˜í™˜
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            Dict: {
                'answer': ìƒì„±ëœ ë‹µë³€,
                'documents': ì°¸ê³  ë¬¸ì„œ ë§í¬ë“¤
            }
        """
        try:
            # 1. FAISS ê²€ìƒ‰
            search_results = self.search_similar_documents(question, top_k=3)
            
            if not search_results:
                return {
                    'answer': "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    'documents': []
                }
            
            # 2. Llamaë¡œ ë‹µë³€ ìƒì„±
            answer = self.generate_answer_with_llama(question, search_results)
            
            # 3. ë¬¸ì„œ ë§í¬ë“¤ ì¶”ì¶œ
            document_links = []
            for result in search_results:
                metadata = result.get('metadata', {})
                if isinstance(metadata, dict) and 'metadata' in metadata:
                    # ë©”íƒ€ë°ì´í„°ê°€ ì¤‘ì²©ëœ êµ¬ì¡°ì¸ ê²½ìš°
                    inner_metadata = metadata['metadata']
                    url = inner_metadata.get('url', '')
                    title = inner_metadata.get('title', 'ì œëª© ì—†ìŒ')
                else:
                    # ì§ì ‘ êµ¬ì¡°ì¸ ê²½ìš°
                    url = metadata.get('url', '')
                    title = metadata.get('title', 'ì œëª© ì—†ìŒ')
                
                if url:
                    # ì™„ì „í•œ URL ìƒì„±
                    if not url.startswith('http'):
                        url = f"https://{url}"
                    
                    document_links.append({
                        'title': title,
                        'url': url,
                        'similarity_score': result.get('similarity_score', 0)
                    })
            
            # 4. ë¡œê¹…
            self.logger.info(f"RAG ë‹µë³€ + ë¬¸ì„œ ìƒì„± ì™„ë£Œ - ì§ˆë¬¸: {question[:50]}...")
            self.logger.info(f"ì°¸ê³  ë¬¸ì„œ {len(document_links)}ê°œ: {[doc['title'][:20] for doc in document_links]}")
            
            return {
                'answer': answer,
                'documents': document_links
            }
            
        except Exception as e:
            self.logger.error(f"RAG ë‹µë³€+ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'answer': "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                'documents': []
            }


def test_rag_system():
    """RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ìƒˆë¡œìš´ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = NewRAGSystem(faiss_dir="./crawling/faiss_output", device_id=0)
    
    if not rag.initialize_all():
        print("âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ì»´í“¨í„°ê³¼í•™ê³¼ ê³¼ì œê°€ ë„ˆë¬´ ì–´ë ¤ì›Œìš”",
        "ì´ì‚°êµ¬ì¡° ìˆ˜ì—…ì€ ì–´ë–¤ê°€ìš”?",
        "ë³µìˆ˜ì „ê³µ ì‹ ì²­ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”"
    ]
    
    print("\nğŸ” RAG ë‹µë³€ í…ŒìŠ¤íŠ¸")
    print("-" * 30)
    
    for question in test_questions:
        print(f"\nì§ˆë¬¸: {question}")
        answer = rag.generate_rag_answer(question)
        print(f"ë‹µë³€: {answer}")
        print("-" * 30)


if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    test_rag_system() 