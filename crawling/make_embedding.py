#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì—ë¸Œë¦¬íƒ€ì„ ì»´í“¨í„°í•™ê³¼ ë°ì´í„°ë¥¼ SFR ëª¨ë¸ë¡œ ì„ë² ë”©í•˜ì—¬ FAISS ì¸ë±ìŠ¤ ìƒì„±
./everytime_computer_data.json íŒŒì¼ì´ ì¡´ì¬í•  ë•Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
"""

import json
import numpy as np
import faiss
import pickle
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import os
import sys
from datetime import datetime

class EverytimeRAGIndexBuilder:
    def __init__(self, model_name="Salesforce/SFR-Embedding-Mistral"):
        """
        ì—ë¸Œë¦¬íƒ€ì„ ë°ì´í„°ë¥¼ ìœ„í•œ FAISS index ë° embedding ìƒì„±ê¸°
        
        Args:
            model_name (str): SFR embedding ëª¨ë¸ ì´ë¦„
        """
        # CUDA ì¥ì¹˜ ì„¤ì • (í•„ìš”ì‹œ ìˆ˜ì •)
        os.environ["CUDA_VISIBLE_DEVICES"] = "0"
        
        self.model_name = model_name
        self.model = None
        self.texts = []
        self.metadata = []
        self.embeddings = None
        self.index = None
        
    def load_model(self):
        """SFR embedding ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸš€ SFR ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
        try:
            self.model = SentenceTransformer(self.model_name)
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
        
    def process_everytime_data(self, json_path):
        """ì—ë¸Œë¦¬íƒ€ì„ JSON ë°ì´í„°ë¥¼ RAG í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í¬ìŠ¤íŠ¸+ëŒ“ê¸€ í†µí•©)"""
        print(f"ğŸ“‚ ì—ë¸Œë¦¬íƒ€ì„ ë°ì´í„° ë¡œë”© ì¤‘: {json_path}")
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = []
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:
                        try:
                            post = json.loads(line)
                            data.append(post)
                        except json.JSONDecodeError as e:
                            print(f"âš ï¸  ë¼ì¸ {line_num}ì—ì„œ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                            continue
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            raise
            
        print(f"ğŸ“Š ì´ {len(data)}ê°œì˜ í¬ìŠ¤íŠ¸ ë°œê²¬")
        
        # ê° í¬ìŠ¤íŠ¸ë¥¼ ëŒ“ê¸€ê³¼ í•¨ê»˜ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        for idx, post in enumerate(data):
            # í¬ìŠ¤íŠ¸ì™€ ëª¨ë“  ëŒ“ê¸€ì„ í†µí•©í•œ í…ìŠ¤íŠ¸ êµ¬ì„±
            combined_text = self._create_post_text(post)
            post_id = f"post_{idx}"
            
            # í†µí•©ëœ í¬ìŠ¤íŠ¸ ì¶”ê°€
            self.texts.append(combined_text)
            self.metadata.append({
                'id': post_id,
                'metadata': {
                    'type': 'post_with_comments',
                    'post_index': idx,
                    'title': post.get('title', ''),
                    'url': post.get('url', ''),
                    'likes': post.get('likes', '0'),
                    'comments_count': post.get('comments_count', '0'),
                    'scraps': post.get('scraps', '0'),
                    'timestamp': post.get('timestamp', ''),
                    'total_comments': len(post.get('comments', []))
                }
            })
                
        print(f"âœ… ë°ì´í„° ë³€í™˜ ì™„ë£Œ! ì´ {len(self.texts)}ê°œì˜ í…ìŠ¤íŠ¸ ìƒì„±")
        print(f"   - í¬ìŠ¤íŠ¸(ëŒ“ê¸€ í¬í•¨): {len(data)}ê°œ")
        
    def _create_post_text(self, post):
        """í¬ìŠ¤íŠ¸ ë°ì´í„°ì™€ ëª¨ë“  ëŒ“ê¸€ì„ í•˜ë‚˜ì˜ ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        title = post.get('title', '')
        detail = post.get('detail', '')
        timestamp = post.get('timestamp', '')
        likes = post.get('likes', '0')
        comments_count = post.get('comments_count', '0')
        
        # ë©”ì¸ í¬ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ êµ¬ì„±
        text = f"ì œëª©: {title}\në‚´ìš©: {detail}\n"
        if timestamp:
            text += f"ì‘ì„±ì‹œê°„: {timestamp}\n"
        text += f"ì¢‹ì•„ìš”: {likes}, ëŒ“ê¸€ìˆ˜: {comments_count}\n"
        
        # ëª¨ë“  ëŒ“ê¸€ì„ í¬ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì— í¬í•¨
        comments = post.get('comments', [])
        if comments:
            text += "\n[ëŒ“ê¸€ë“¤]\n"
            for comment_idx, comment in enumerate(comments, 1):
                comment_text = comment.get('Comment', '')
                author = comment.get('Author', '')
                comment_timestamp = comment.get('Timestamp', '')
                vote_count = comment.get('Vote Count', '0')
                comment_type = comment.get('Type', '')
                parent_author = comment.get('Parent Author', '')
                
                # ëŒ“ê¸€ ì •ë³´ ì¶”ê°€
                text += f"\nëŒ“ê¸€{comment_idx}"
                if comment_type == 'child':
                    text += f"(ë‹µê¸€ to {parent_author})"
                text += f": {comment_text}"
                
                if author:
                    text += f" - {author}"
                if comment_timestamp:
                    text += f" ({comment_timestamp})"
                if vote_count and vote_count != '0':
                    text += f" [ì¶”ì²œ:{vote_count}]"
        
        return text
        
    def create_embeddings(self, batch_size=32):
        """í…ìŠ¤íŠ¸ë¥¼ embeddingìœ¼ë¡œ ë³€í™˜"""
        print("ğŸ”„ Embedding ìƒì„± ì¤‘...")
        
        if self.model is None:
            self.load_model()
            
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ embedding ìƒì„±
        embeddings_list = []
        
        for i in tqdm(range(0, len(self.texts), batch_size), desc="Embedding ì§„í–‰"):
            batch_texts = self.texts[i:i+batch_size]
            try:
                batch_embeddings = self.model.encode(batch_texts, show_progress_bar=False)
                embeddings_list.extend(batch_embeddings)
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                raise
                
        self.embeddings = np.array(embeddings_list)
        print(f"âœ… Embedding ìƒì„± ì™„ë£Œ! Shape: {self.embeddings.shape}")
        
    def create_faiss_index(self):
        """FAISS index ìƒì„±"""
        print("ğŸ” FAISS index ìƒì„± ì¤‘...")
        
        if self.embeddings is None:
            raise ValueError("ë¨¼ì € embeddingì„ ìƒì„±í•´ì£¼ì„¸ìš”!")
            
        # L2 ê±°ë¦¬ ê¸°ë°˜ FAISS index ìƒì„±
        dimension = self.embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        
        # embedding ì¶”ê°€
        self.index.add(self.embeddings.astype('float32'))
        
        print(f"âœ… FAISS index ìƒì„± ì™„ë£Œ! ì´ {self.index.ntotal}ê°œ ë²¡í„° ì¶”ê°€ë¨")
        
    def save_index_and_data(self, output_dir="./faiss_output"):
        """FAISS indexì™€ ê´€ë ¨ ë°ì´í„° ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘: {output_dir}")
        
        # FAISS index ì €ì¥
        index_path = os.path.join(output_dir, "faiss_index.bin")
        faiss.write_index(self.index, index_path)
        print(f"  ğŸ“ FAISS index ì €ì¥ë¨: {index_path}")
        
        # í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥
        texts_path = os.path.join(output_dir, "texts.pkl")
        with open(texts_path, 'wb') as f:
            pickle.dump(self.texts, f)
        print(f"  ğŸ“ í…ìŠ¤íŠ¸ ë°ì´í„° ì €ì¥ë¨: {texts_path}")
        
        metadata_path = os.path.join(output_dir, "metadata.pkl")
        with open(metadata_path, 'wb') as f:
            pickle.dump(self.metadata, f)
        print(f"  ğŸ“ ë©”íƒ€ë°ì´í„° ì €ì¥ë¨: {metadata_path}")
        
        # ì„ë² ë”© ì €ì¥ (ì˜µì…˜)
        embeddings_path = os.path.join(output_dir, "embeddings.npy")
        np.save(embeddings_path, self.embeddings)
        print(f"  ğŸ“ Embedding ë°ì´í„° ì €ì¥ë¨: {embeddings_path}")
        
        # ì„¤ì • ì •ë³´ ì €ì¥
        config = {
            'model_name': self.model_name,
            'num_texts': len(self.texts),
            'embedding_dimension': self.embeddings.shape[1],
            'created_at': datetime.now().isoformat(),
            'source_file': './everytime_computer_data.json',
            'data_type': 'everytime_computer_science_posts'
        }
        
        config_path = os.path.join(output_dir, "config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        print(f"  ğŸ“ ì„¤ì • ì •ë³´ ì €ì¥ë¨: {config_path}")


def check_input_file():
    """ì…ë ¥ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    input_file = "./everytime_computer_data.json"
    
    if not os.path.exists(input_file):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        print("   ./everytime_computer_data.json íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False
        
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size = os.path.getsize(input_file)
    if file_size == 0:
        print(f"âŒ ì…ë ¥ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {input_file}")
        return False
        
    print(f"âœ… ì…ë ¥ íŒŒì¼ í™•ì¸ë¨: {input_file} ({file_size:,} bytes)")
    return True


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ì—ë¸Œë¦¬íƒ€ì„ ì»´í“¨í„°í•™ê³¼ ë°ì´í„° ì„ë² ë”© ìƒì„±ê¸°")
    print("=" * 60)
    
    # 1. ì…ë ¥ íŒŒì¼ í™•ì¸
    if not check_input_file():
        sys.exit(1)
    
    # ê²½ë¡œ ì„¤ì •
    input_file = "./everytime_computer_data.json"
    output_dir = "faiss_output"
    
    # RAG Index Builder ì´ˆê¸°í™”
    builder = EverytimeRAGIndexBuilder()
    
    try:
        # 2. ì—ë¸Œë¦¬íƒ€ì„ ë°ì´í„° ë¡œë“œ ë° ë³€í™˜
        builder.process_everytime_data(input_file)
        
        # 3. ëª¨ë¸ ë¡œë“œ
        builder.load_model()
        
        # 4. Embedding ìƒì„±
        print("\n" + "=" * 60)
        builder.create_embeddings(batch_size=16)  # ë©”ëª¨ë¦¬ ìƒí™©ì— ë”°ë¼ ì¡°ì ˆ ê°€ëŠ¥
        
        # 5. FAISS index ìƒì„±
        print("\n" + "=" * 60)
        builder.create_faiss_index()
        
        # 6. ê²°ê³¼ ì €ì¥
        print("\n" + "=" * 60)
        builder.save_index_and_data(output_dir)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ RAG Index ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
        print(f"ğŸ“Š ì´ í…ìŠ¤íŠ¸ ìˆ˜: {len(builder.texts):,}")
        print(f"ğŸ”¢ Embedding ì°¨ì›: {builder.embeddings.shape[1]}")
        print(f"ğŸ’¾ ì¸ë±ìŠ¤ í¬ê¸°: {builder.index.ntotal:,} ë²¡í„°")
        
        # ìƒì„±ëœ íŒŒì¼ë“¤ í™•ì¸
        print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
        for filename in ['faiss_index.bin', 'texts.pkl', 'metadata.pkl', 'embeddings.npy', 'config.json']:
            filepath = os.path.join(output_dir, filename)
            if os.path.exists(filepath):
                size = os.path.getsize(filepath)
                print(f"   - {filename}: {size:,} bytes")
        
        print("\nâœ… ì²˜ë¦¬ ì™„ë£Œ! ì´ì œ RAG ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
        
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 